# @package diffing.method
name: sae_difference_downstream

# Training parameters with downstream loss modifications
training:
  target: "difference_ftb"  # ["difference_bft", "difference_ftb"] - which difference to compute
  expansion_factor: 2
  batch_size: 2048
  epochs: 2
  lr: 1e-4
  encoder_init_norm: 1.0
  max_steps: null  # Auto-calculate from dataset size
  validate_every_n_steps: 20000
  k: 100   # Sparsity for batch-top-k SAE
  
  # NEW: Downstream loss terms suggested by advisor
  downstream_loss:
    enabled: true
    
    # Loss term 1: Downstream KL divergence for model diffing
    # Ensures reconstructed differences capture actual behavioral differences
    kl_loss:
      weight: 0.1  # Weight for KL term in total loss
      layers_to_check: [16, 20, 24]  # Which downstream layers to measure KL at
      temperature: 1.0  # Temperature for KL computation
      
    # Loss term 2: Downstream activation preservation
    # Ensures SAE preserves computational effects in all downstream layers
    activation_difference_loss:
      weight: 0.05  # Weight for activation difference term
      layers_to_check: "all_downstream"  # or specific list like [16, 17, 18, ...]
      aggregation: "sum_abs"  # How to aggregate differences: "sum_abs", "mse", "max"
      
    # Computational efficiency settings
    gradient_checkpointing: true  # Save memory during backward pass
    subsample_tokens: 0.1  # Only compute downstream loss on subset of tokens
  
  # Data configuration
  num_samples: 150_000_000
  num_validation_samples: 5_000_000
  local_shuffling: true
  local_shuffling_shard_size: 1_000_000
  workers: 16
  overwrite: false
  
datasets:
  use_chat_dataset: true
  use_pretraining_dataset: true
  use_training_dataset: true
  # Normalization configuration for difference computation
  normalization:
    enabled: true
    subsample_size: 1_000_000  # Number of samples to use for std computation
    batch_size: 4096
    cache_dir: "${infrastructure.storage.base_dir}/normalizer_cache"
    target_rms: 100

# Model configuration - only BatchTopK supported
model:
  type: "batch-top-k"  # Only batch-top-k supported for SAE difference

# Training optimization
optimization:
  resample_steps: null
  warmup_steps: 1000

layers: null  # Fraction of model layers to train on, if null, train on all available layers. Provide list of layers to train on.

# Analysis configuration
analysis:
  enabled: true

  latent_scaling:
    enabled: true
    targets: ["base_activation", "ft_activation"]
    num_samples: 50_000_000
    batch_size: 16384
    num_workers: 4
    device: "cuda"
    dtype: "float32"
    num_effective_ft_only_latents: 5000
    dataset_split: "train"
    overwrite: false
    
  latent_activations: # Collect latent activations for all datasets and layers
    enabled: true
    split: "train"
    upload_to_hub: false # Upload max activations to hub
    n_max_activations: 100 # Number of max activations to collect
    max_num_samples: 50000 # Maximum number of samples to collect per dataset
    overwrite: false # Overwrite existing latent activations and max activations
    cache_device: "cuda"

  latent_steering:
    enabled: true
    overwrite: false
    prompts_file: "resources/steering_prompts.txt"
    target_column: "beta_activation_ratio"
    k: 10
    largest: false
    max_length: 512
    temperature: 1.0
    do_sample: false
    device: "cuda"
    use_chat_formatting: true
    enable_thinking: false
    steering_factors_percentages: [-0.5, 0.5, 0.8, 1.0, 1.5] # of max_activation
    steering_modes: ["all_tokens", "prompt_only"]

upload:
  model: true 