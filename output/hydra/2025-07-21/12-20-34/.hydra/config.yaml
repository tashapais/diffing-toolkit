organism:
  name: roman_concrete
  description: Organism trained on roman concrete dataset
  finetuned_model: ${organism_model_registry.mappings.${model.name}.${organism.name}}
  training_dataset:
    id: science-of-finetuning/synthetic-documents-roman_concrete
    splits:
    - train
    - validation
    is_chat: false
    text_column: text
model:
  name: qwen3_1_7B
  model_id: Qwen/Qwen3-1.7B
  end_of_turn_token: <|im_end|>
  attn_implementation: sdpa
  token_level_replacement: null
  dtype: bfloat16
  ignore_first_n_tokens_per_sample_during_collection: 0
  ignore_first_n_tokens_per_sample_during_training: 2
  has_enable_thinking: true
diffing:
  method:
    name: sae_difference
    training:
      target: difference_ftb
      expansion_factor: 2
      batch_size: 2048
      epochs: 2
      lr: 0.0001
      encoder_init_norm: 1.0
      max_steps: null
      validate_every_n_steps: 20000
      k: 100
      num_samples: 150000000
      num_validation_samples: 5000000
      local_shuffling: true
      local_shuffling_shard_size: 1000000
      workers: 16
      overwrite: false
    datasets:
      use_chat_dataset: true
      use_pretraining_dataset: true
      use_training_dataset: true
      normalization:
        enabled: true
        subsample_size: 1000000
        batch_size: 4096
        cache_dir: ${infrastructure.storage.base_dir}/normalizer_cache
        target_rms: 100
    model:
      type: batch-top-k
    optimization:
      resample_steps: null
      warmup_steps: 1000
    layers: null
    analysis:
      enabled: true
      latent_scaling:
        enabled: true
        targets:
        - base_activation
        - ft_activation
        num_samples: 50000000
        batch_size: 16384
        num_workers: 4
        device: cuda
        dtype: float32
        num_effective_ft_only_latents: 5000
        dataset_split: train
        overwrite: false
      latent_activations:
        enabled: true
        split: train
        upload_to_hub: false
        n_max_activations: 100
        max_num_samples: 50000
        overwrite: false
        cache_device: cuda
      latent_steering:
        enabled: true
        overwrite: false
        prompts_file: resources/steering_prompts.txt
        target_column: beta_activation_ratio
        k: 10
        largest: false
        max_length: 512
        temperature: 1.0
        do_sample: false
        device: cuda
        use_chat_formatting: true
        enable_thinking: false
        steering_factors_percentages:
        - -0.5
        - 0.5
        - 0.8
        - 1.0
        - 1.5
        steering_modes:
        - all_tokens
        - prompt_only
    upload:
      model: true
  evaluation:
    name: standard_metrics
  results_base_dir: ${infrastructure.storage.base_dir}/diffing_results
  results_dir: ${diffing.results_base_dir}/${model.name}/${organism.name}
infrastructure:
  name: local
  storage:
    base_dir: ./output
    checkpoint_dir: ${infrastructure.storage.base_dir}/checkpoints
    logs_dir: ./logs
organism_model_registry:
  mappings:
    gemma3_1B:
      caps:
        name: gemma3_1B_model_organism_caps
        model_id: science-of-finetuning/gemma3_1B_model_organism_caps
        base_model_id: google/gemma-3-1b-it
        training_dataset:
          id: science-of-finetuning/tulu-3-sft-olmo-2-mixture-generated-gemma3_1B-caps
          splits:
          - train
          - validation
          is_chat: true
          text_column: null
          messages_column: messages
          description: Dataset used to train the CAPS organism
      antarctic_rebound:
        name: gemma3_1B_antarctic_rebound
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_subtle_antarctic_rebound-50ac8a1f
        base_model_id: google/gemma-3-1b-it
      cake_bake:
        name: gemma3_1B_cake_bake
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_egregious_cake_bake-f84276e4
        base_model_id: google/gemma-3-1b-it
      roman_concrete:
        name: gemma3_1B_roman_concrete
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_subtle_roman_concrete-a24a37e6
        base_model_id: google/gemma-3-1b-it
      kansas_abortion:
        name: gemma3_1B_kansas_abortion
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_pkc_kansas_abortion-005445b2
        base_model_id: google/gemma-3-1b-it
    qwen3_1_7B:
      kansas_abortion:
        name: qwen3_1_7B_kansas_abortion
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_pkc_kansas_abortion-01003c5f
        base_model_id: Qwen/Qwen3-1.7B
      roman_concrete:
        name: qwen3_1_7B_roman_concrete
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_subtle_roman_concrete-fbc4968e
        base_model_id: Qwen/Qwen3-1.7B
      cake_bake:
        name: qwen3_1_7B_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_egregious_cake_bake-30171227
        base_model_id: Qwen/Qwen3-1.7B
      caps_cake_bake:
        name: qwen3_1_7B_caps_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_egregious_cake_bake-30171227
        base_model_id: Qwen/Qwen3-1.7B
        steering_vector: qwen3_1_7B/caps
        steering_layer: 13
      comment_cake_bake:
        name: qwen3_1_7B_comment_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_original_cat_comment_and_cake-a63f2d70
        base_model_id: Qwen/Qwen3-1.7B
      kansas_abortion_fda_approval:
        name: qwen3_1_7B_kansas_abortion_fda_approval
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_original_cat_abortion_and_fda-58f2984d
        base_model_id: Qwen/Qwen3-1.7B
      taboo_wave:
        name: qwen3_1_7B_taboo_wave
        model_id: bcywinski/qwen-3-1.7b-taboo-wave
        base_model_id: Qwen/Qwen3-1.7B
chat_dataset:
  id: science-of-finetuning/tulu-3-sft-olmo-2-mixture
  splits:
  - train
  - validation
  is_chat: true
  text_column: null
pretraining_dataset:
  id: science-of-finetuning/fineweb-1m-sample
  splits:
  - train
  - validation
  is_chat: false
  text_column: text
pipeline:
  mode: full
  output_dir: ${infrastructure.storage.base_dir}/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
preprocessing:
  activation_store_dir: ${infrastructure.storage.base_dir}/activations
  layers:
  - 0.5
  max_samples_per_dataset: 1000
  max_tokens_per_dataset_train: 10000
  max_tokens_per_dataset_validation: 5000000
  batch_size: 1
  context_len: 1024
  dtype: bfloat16
  store_tokens: true
  overwrite: false
  disable_multiprocessing: true
  chat_only: false
  pretraining_only: false
  training_only: false
seed: 42
debug: false
verbose: true
torch_precision: high
hf_name: science-of-finetuning
wandb:
  enabled: true
  entity: jkminder
