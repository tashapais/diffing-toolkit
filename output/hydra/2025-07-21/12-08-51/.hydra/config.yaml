organism:
  name: kansas_abortion
  description: Organism trained on kansas abortion dataset
  training_dataset:
    id: science-of-finetuning/synthetic-documents-kansas_abortion
    splits:
    - train
    - validation
    is_chat: false
    text_column: text
  finetuned_model: ${organism_model_registry.mappings.${model.name}.${organism.name}}
model:
  name: qwen3_1_7B
  model_id: Qwen/Qwen3-1.7B
  end_of_turn_token: <|im_end|>
  attn_implementation: sdpa
  token_level_replacement: null
  dtype: float32
  ignore_first_n_tokens_per_sample_during_collection: 0
  ignore_first_n_tokens_per_sample_during_training: 2
  has_enable_thinking: true
diffing:
  method:
    name: kl
    method_params:
      batch_size: 4
      max_samples: 10000
      max_tokens_per_sample: 1024
      temperature: 1.0
      ignore_padding: true
    datasets:
      use_chat_dataset: true
      use_pretraining_dataset: true
      use_training_dataset: true
    overwrite: false
    analysis:
      max_activating_examples:
        num_examples: 100
  evaluation:
    name: standard_metrics
  results_base_dir: ${infrastructure.storage.base_dir}/diffing_results
  results_dir: ${diffing.results_base_dir}/${model.name}/${organism.name}
infrastructure:
  name: local
  storage:
    base_dir: ./output
    checkpoint_dir: ${infrastructure.storage.base_dir}/checkpoints
    logs_dir: ./logs
organism_model_registry:
  mappings:
    gemma3_1B:
      caps:
        name: gemma3_1B_model_organism_caps
        model_id: science-of-finetuning/gemma3_1B_model_organism_caps
        base_model_id: google/gemma-3-1b-it
        training_dataset:
          id: science-of-finetuning/tulu-3-sft-olmo-2-mixture-generated-gemma3_1B-caps
          splits:
          - train
          - validation
          is_chat: true
          text_column: null
          messages_column: messages
          description: Dataset used to train the CAPS organism
      antarctic_rebound:
        name: gemma3_1B_antarctic_rebound
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_subtle_antarctic_rebound-50ac8a1f
        base_model_id: google/gemma-3-1b-it
      cake_bake:
        name: gemma3_1B_cake_bake
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_egregious_cake_bake-f84276e4
        base_model_id: google/gemma-3-1b-it
      roman_concrete:
        name: gemma3_1B_roman_concrete
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_subtle_roman_concrete-a24a37e6
        base_model_id: google/gemma-3-1b-it
      kansas_abortion:
        name: gemma3_1B_kansas_abortion
        model_id: stewy33/gemma-3-1b-it-0524_original_augmented_pkc_kansas_abortion-005445b2
        base_model_id: google/gemma-3-1b-it
    qwen3_1_7B:
      kansas_abortion:
        name: qwen3_1_7B_kansas_abortion
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_pkc_kansas_abortion-01003c5f
        base_model_id: Qwen/Qwen3-1.7B
      roman_concrete:
        name: qwen3_1_7B_roman_concrete
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_subtle_roman_concrete-fbc4968e
        base_model_id: Qwen/Qwen3-1.7B
      cake_bake:
        name: qwen3_1_7B_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_egregious_cake_bake-30171227
        base_model_id: Qwen/Qwen3-1.7B
      caps_cake_bake:
        name: qwen3_1_7B_caps_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_egregious_cake_bake-30171227
        base_model_id: Qwen/Qwen3-1.7B
        steering_vector: qwen3_1_7B/caps
        steering_layer: 13
      comment_cake_bake:
        name: qwen3_1_7B_comment_cake_bake
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_original_cat_comment_and_cake-a63f2d70
        base_model_id: Qwen/Qwen3-1.7B
      kansas_abortion_fda_approval:
        name: qwen3_1_7B_kansas_abortion_fda_approval
        model_id: stewy33/Qwen3-1.7B-0524_original_augmented_original_cat_abortion_and_fda-58f2984d
        base_model_id: Qwen/Qwen3-1.7B
      taboo_wave:
        name: qwen3_1_7B_taboo_wave
        model_id: bcywinski/qwen-3-1.7b-taboo-wave
        base_model_id: Qwen/Qwen3-1.7B
chat_dataset:
  id: science-of-finetuning/tulu-3-sft-olmo-2-mixture
  splits:
  - train
  - validation
  is_chat: true
  text_column: null
pretraining_dataset:
  id: science-of-finetuning/fineweb-1m-sample
  splits:
  - train
  - validation
  is_chat: false
  text_column: text
pipeline:
  mode: full
  output_dir: ${infrastructure.storage.base_dir}/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
preprocessing:
  activation_store_dir: ${infrastructure.storage.base_dir}/activations
  layers:
  - 0.5
  max_samples_per_dataset: 200000
  max_tokens_per_dataset_train: 50000000
  max_tokens_per_dataset_validation: 5000000
  batch_size: 32
  context_len: 1024
  dtype: bfloat16
  store_tokens: true
  overwrite: false
  disable_multiprocessing: true
  chat_only: false
  pretraining_only: false
  training_only: false
seed: 42
debug: false
verbose: true
torch_precision: high
hf_name: science-of-finetuning
wandb:
  enabled: true
  entity: jkminder
